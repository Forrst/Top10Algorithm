朴素贝叶斯分类器被广泛运用于文本分类等领域，甚至与SVM对比起来，它也很具竞争性。

朴素贝叶斯，顾名思义，运用贝叶斯定理，考虑了先验概率，假设各个特征间是相互独立的。特征的独立性在现实生活中未必成立，所以理论上朴素贝叶斯具有最高的准确率，但是实际应用未必是这样。朴素贝叶斯经常拿来和决策树做对比，有朴素贝叶斯的假设可以知道，当特征之间的相关性低的时候，朴素贝叶斯的分类效果会比决策树好，反之则决策树的效果更好。

朴素贝叶斯模型数学原理
贝叶斯公式： p(A|B) = [p(B|A) * p(A)] / p(B)
用自然语言说就是B发生的前提下，A发生的概率等于发生A的概率乘A发生的前提下B发生的概率，除以B发生的概率。
设C为类别,F1...Fn为特征,则有贝叶斯公式有:
p(C|F1...Fn) = [p(C) * p(F1...Fn|C)] / p(F1, ..., Fn)
由联合概率分布性质:
p(C, F1, ..., Fn) = p(C) * p(F1|C) * ... * p(Fn|F1,...,Fn-1, C)
得:
p(C|F1, ..., Fn) = p(C, F1, ..., Fn) / p(F1, ..., Fn)
目的就是要对于特定的样本F1 = f1,...,Fn = fn,确定C = Ci,使得p(C|F1, ..., Fn)最大.所以分母项可视为常数Z.
由于各特征间相互独立,所以有
p(C|F1 , ..., Fn) = p(C) * p(F1|C) *...* p(Fn|C) / Z

朴素贝叶斯分类器模型
classify(f1...fn) = argmax p(C = c) * p(F1 = f1|C = c) *...* p(Fn = fn|C = c)


多项式朴素贝叶斯
由上文可知,p(C|F1 , ..., Fn) = p(C) * p(F1|C) *...* p(Fn|C) / Z
假设p(Fi|Ci)服从多项式分布,用极大似然估计估计其概率.
我们以文本分类做例子.
假设Ci中有n个词语,而在属于Ci的文章中,Fi出现了k次,由于其服从多项式分布,所以
p(Fi = k|Ci) = {n!/[k! * (n - k)!]} * p^k * (1-p)^(n-k)
其中p为词Fi出现的概率
对上面式子的p求导并令其等于零,解得p再代入求出p(Fi = k|Ci).
p(Ci)为属于Ci的文章占总文章的比例.
算出各个p(C = Ci|F)取最大的类作为分类.

伯努利朴素贝叶斯
伯努利朴素贝叶斯也是处理离散型特征的模型。和多项式朴素贝叶斯不同的是，伯努利朴素贝叶斯假设p(Fi|Ci)服从伯努利分布。其中Fi代表特征i有没有出现，出现则为1，不然则为零。对p的估计也是用极大似然估计。
由伯努利分布的pmf：
		   p     if k = 1
	pmf = 
		   (1-p) if k = 0
我们可以把两条式子合在一起:
	pmf = k*(p) + (1-k)*(1-p)	k = 1 or 0
除了分布和特征计算方法和多项式不一样，其他都一样，就不再赘述了

高斯朴素贝叶斯
高斯朴素贝耶斯是处理连续性特征的朴素贝叶斯模型，其假设p(Fi|Ci)服从正态分布。
正态分布中有两个参数是我们要从训练集中获取的：均值和标准差，采用方法依旧是极大似然估计。
原理上和以上两种没有区别。

总的来说，三种模型原理上都是用了贝叶斯公式，考虑了变量的先验分布。




